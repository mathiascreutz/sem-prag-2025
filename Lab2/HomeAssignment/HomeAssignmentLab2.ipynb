{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 home assignment: Distributional semantics\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"red\">**This page contains interactive graphics. It only works properly if you use the \"classic notebook\" user interface. If you did not do that yet, start by selecting *Launch Classic Notebook* from the *Help* menu.**</font>\n",
    "\n",
    "---\n",
    "\n",
    "Your homework consists of three tasks, worth 4 points combined. Add your solutions to this notebook page. When you are done, download the page as a notebook file (.ipynb) and submit that file on Moodle.\n",
    "\n",
    "---\n",
    "\n",
    "### Task 1: Semantic feature analysis of emotions\n",
    "\n",
    "*You can get a maximum of 1.5 points from this task.*\n",
    "\n",
    "Your first task concerns *sentiment analysis*, or more specifically what connotations of *emotions* words carry. According to the American psychologist [Robert Plutchik](https://en.wikipedia.org/wiki/Robert_Plutchik) (1927 – 2006), there are eight primary emotions: **anger**, **fear**, **sadness**, **disgust**, **surprise**, **anticipation**, **trust**, and **joy**.\n",
    "\n",
    "In Plutchik's theory, more complex emotions are derived from the eight primary emotions by combining different intensities of the primary emotions.\n",
    "\n",
    "For instance, the word \"*reward*\" involves the emotions *anticipation*, *joy*, *surprise*, and *trust* to different degrees. The word \"*worry*\" involves *anticipation*, *fear*, and *sadness*. The word \"*suddenly*\" is only about *surprise*, whereas the word \"*garbage*\" triggers an emotion of *disgust*.\n",
    "\n",
    "**Task 1.1:** `(1 point)` Your task is to do a semantic feature analysis for a set of words. As semantic features you should use Plutchik's eight primary emotions. You should perform dimensionality reduction and plot the word vectors you obtain in both 2D and 3D, exactly as in the _games_ example in Part 2 of the class assignment. Also include a table of the pairwise angles between all vectors.\n",
    "\n",
    "However, first you need to import the necessary plot utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append(\"../../../sem-prag-2025/src\")\n",
    "import plot_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your analysis, pick *10 words* at your own choice from the following list of words: `anaconda, bucket, concert, dirty, etymology, forsaken, gain, hairy, instruction, judging, kind, lustful, monster, night, overestimate, profit, quick, refund, summer, tolerate, underpants, vinegar, warn, xenophobia, yesterday, zodiac`.\n",
    "\n",
    "In the code cell below, produce eight-dimensional vectors for the ten words you have selected. For every word, go through all eight primary emotions. Depending on what emotions the words convey to you, set feature values as follows: Use a value of zero (0) if the emotion is totally absent, and one (1) for a strong presense of that particular emotion. You can also use degrees between zero and one for some of the feature dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: (1) anger, (2) fear, (3) sadness, (4) disgust,\n",
    "#           (5) surprise, (6) anticipation, (7) trust, (8) joy\n",
    "\n",
    "words = [            # (1)   (2)   (3)   (4)   (5)   (6)   (7)   (8)\n",
    "    (\"word1\",        [ 0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0]),  # modify these!\n",
    "    (\"word2\",        [ 0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0]),\n",
    "    (\"word3\",        [ 0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  0.0]),\n",
    "    # etc.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next perform dimensionality reduction and plot in two and three dimensions. Also produce the table of pairwise angles between the word vectors. (If the plot does not work as it should, go back to Part 1 of the class assignment to see what to do about that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_2d_binary_hd(words, arrows=False)\n",
    "plot_utils.plot_3d_binary_hd(words, arrows=True)\n",
    "plot_utils.tabulate_angles(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2:** `(0.5 points)` When you have produced the ten word vectors, have plotted them and counted the angles between the vectors, please answer the following *questions*:\n",
    "\n",
    "1. Looking at the pairwise angles, which two words are *most similar* and which two words are *least similar* to each other based on the emotion features? Does this make sense?\n",
    "\n",
    "2. Compared to the exactly measured angles, do the two- and three-dimensional projections of the vectors reflect the similarities and dissimilarities accurately? That is, in comparison to the measured angles, do the plotted figures give you the same impression of which words are most similar and which words are least similar? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer to question 1:\n",
    "#\n",
    "#\n",
    "\n",
    "# Your answer to question 2:\n",
    "#\n",
    "#\n",
    "\n",
    "# Put a hashtag (#) at the beginning of every line of your answer, so\n",
    "# that your text is treated as comments rather than Python commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Task 2: Word contexts of related and unrelated words\n",
    "\n",
    "*You can get a maximum of 1.5 points from this task. You need to answer 5 questions, and each question is worth 0.3 points.*\n",
    "\n",
    "Before proceeding to Tasks 2 and 3, you need to import some further libraries and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import two book texts from the Gutenberg corpus: Moby Dick and Sense and Sensibility\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "moby_dick = Text(gutenberg.words(\"melville-moby_dick.txt\"))\n",
    "sense_and_sensibility = Text(gutenberg.words(\"austen-sense.txt\"))\n",
    "\n",
    "# Import some additional auxiliary libraries\n",
    "import distribsem\n",
    "import numpy as np\n",
    "\n",
    "# These lines filter out some characters from the texts to make it less noisy\n",
    "moby_dick = distribsem.filter_text(moby_dick)\n",
    "sense_and_sensibility = distribsem.filter_text(sense_and_sensibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second task is to study the contexts of different words, keeping in mind the ideas about learning word embeddings from corpora. You can read slides 55 – 58 from Lecture 2 if you need a refresher on this. We also remind you of some basics here.\n",
    "\n",
    "When learning word embeddings from a corpus, there are a few parameters we need to decide on beforehand. First of all, we need to define a window size for the context we want to use. For example, a window of +/- 4 words means we will consider as context the four words before and the four words after the focus word.\n",
    "\n",
    "We also need to decide the dimensionality of the final vectors. In our case this means the size of the context vocabulary we want to take into account. We can decide, for example, to use the 1000 most common words in the corpus as context vocabulary. Consequently, our embeddings will be 1000-dimensional. Any words outside the top 1000 will simply not be taken into account.\n",
    "\n",
    "In the code cell below, you are given a function `show_kwic` (kwic = **key-word in context**), that you can use to retrieve instances of a word of your choice in a corpus. The example shows the 10 first occurrences of the word *water* in *Moby Dick*, using a window size of +/- 4 and the 500 most common words as context vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "**Your task:** Pick **one pair** of *closely related* words (for example, *sky* and *cloud* can be considered closely related) and **one pair** of *unrelated* words (for example, *house* and *jump*). Study the contexts of the words and see how/whether the (un)relatedness is manifested in the contexts. Answer as comments in the code cell.\n",
    "\n",
    "You should also try different values for `dimensionality` and `window_size`. `dimensionality` controls the size of the context vocabulary (using top frequent words) and `window_size` controls the window size of the context. Try `dimensionality` values in different ranges (tens, hundreds, thousands) and vary the window size for example between 1 and 10. Write answers to the following questions as comments in the code cell below:\n",
    "\n",
    "1. Which pairs of words did you pick as the two related words and the two unrelated words?\n",
    "2. How does dimensionality affect how easy it is to see the (un)relatedness of the words?\n",
    "3. How does window size affect how easy it is to see the (un)relatedness of the words?\n",
    "4. What kinds of characteristics do you think the embeddings capture with small window sizes and dimensionalities?\n",
    "5. What kinds of characteristics do you think the embeddings capture with large window sizes and dimensionalities?\n",
    "\n",
    "You can use either the text `moby_dick` or `sense_and_sensibility`.\n",
    "\n",
    "**Note:** *Words outside the top \"dimensionality\" frequent words are replaced with UNKs in the kwics.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "source_text = moby_dick   # alternatively: sense_and_sensibility\n",
    "focus_word = \"water\"      # use different words\n",
    "window_size = 4           # try values between 1 and 10\n",
    "dimensionality = 500      # try values in the range of tens, hundreds, thousands\n",
    "show_n_occurrences = 10   # increase this if you like\n",
    "\n",
    "distribsem.show_kwic(\n",
    "    text=source_text,\n",
    "    word=focus_word,\n",
    "    window=window_size,\n",
    "    dimensionality=dimensionality,\n",
    "    show_n=show_n_occurrences\n",
    ")\n",
    "\n",
    "# Answer questions 1 – 5 as comments in this code cell:\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Task 3: Comparing embeddings across corpora\n",
    "\n",
    "*You can get a maximum of 1 point from this task.*\n",
    "\n",
    "In this third task we will continue the theme of word embeddings and contexts. This time we will study embeddings trained on two different texts. The texts we will use are the familiar *Moby Dick* (variable `moby_dick`) and *Sense and Sensibility* (variable `sense_and_sensibility`). \n",
    "\n",
    "In the code cell below we train embeddings for a set of words on both texts (function `create_vectors_shared`). This will result in two embedding matrices (`M_moby_shared` and `M_sense_shared`) as well as a mapping from words to their row indices (`mapping_shared`). The vocabulary size and embedding dimensionalities are a bit weird because of the way they are restricted to get comparable embeddings. (Don't worry about them.)\n",
    "\n",
    "Run the code cell and read on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M_moby_shared, M_sense_shared, mapping_shared = distribsem.create_vectors_shared(\n",
    "    max_vocab_size=10000,\n",
    "    min_dimensionality=1000,\n",
    "    window_size=4,\n",
    "    text1=moby_dick,\n",
    "    text2=sense_and_sensibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below we show how you can visualize the word embeddings. Embeddings trained on different texts are color-coded. \n",
    "\n",
    "**Task 3.1:** `(0.5 points)` Your first task is to find one word that has relatively similar embeddings in the two texts (meaning the two embeddings for the *same word* are close to each other in the figure) as well as one word that has completely different embeddings (again, a case where the same word gets two very different embeddings in the two texts). Plot the two words like in the example code and include comments where you explain which words are close and which not. \n",
    "\n",
    "**Do not use the words *water* or *man* that have been supplied.** Also, don't change the window size or dimensionality for this task.\n",
    "\n",
    "Read on to **Task 3.2** after you have done this part of the task.\n",
    "\n",
    "**Note:** *Use your best judgment on what \"relatively close\" means. It is hard to find words that have completely identical embeddings or words that have embeddings that are on completely opposite regions of the vector space. If you run the code cell below, you can see two word visualized, \"water\" and \"man\". \"Water\" is a good example of dissimilar embeddings and \"man\" is an example of a word with two similar embeddings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "focus_words = \"water man\".split()  # replace this with two other words!\n",
    "\n",
    "distribsem.plot_two_embeddings(\n",
    "    words=focus_words,\n",
    "    embeddings_1=M_moby_shared,\n",
    "    embeddings_2=M_sense_shared,\n",
    "    mapping_1=mapping_shared,\n",
    "    embeddings_1_name=\"Moby Dick\",\n",
    "    embeddings_2_name=\"Sense and Sensibility\"\n",
    ")\n",
    "\n",
    "# Task 3.1: Answer the questions here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2:** `(0.5 points)` Your second task is to pick the two words you found in **Task 3.1**: one with similar embeddings and the other one with dissimilar embeddings. Analyze the contexts of the two words in the two texts, similar to what you did in Task 2. How do the (dis)similarities of the embeddings show in the word contexts in different texts? Again, answer the question in your code. \n",
    "\n",
    "You do not need to change the window size of dimensionality in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n_occurrences = 20\n",
    "focus_word = \"water\"      # change this!\n",
    "\n",
    "print(\"Occurrences in Sense and Sensibility:\")\n",
    "distribsem.show_kwic(\n",
    "    text=sense_and_sensibility,\n",
    "    word=focus_word,\n",
    "    window=4,\n",
    "    dimensionality=1000,\n",
    "    show_n=show_n_occurrences\n",
    ")\n",
    "\n",
    "print(\"\\nOccurrences in Moby Dick:\")\n",
    "distribsem.show_kwic(\n",
    "    text=moby_dick,\n",
    "    word=focus_word,\n",
    "    window=4,\n",
    "    dimensionality=1000,\n",
    "    show_n=show_n_occurrences\n",
    ")\n",
    "\n",
    "# Task 3.2: Answer the questions here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download this page as a notebook file (.ipynb) and submit it through Moodle before the deadline. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
