{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 home assignment: Similarity and clustering\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"red\">**This page contains interactive graphics. It only works properly if you change to the \"classic notebook\" user interface. Start by selecting *Launch Classic Notebook* from the *Help* menu.**</font>\n",
    "\n",
    "---\n",
    "\n",
    "Run the code cell below to import some necessary functions and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append(\"../../../sem-prag-2025/src\")\n",
    "import plot_utils\n",
    "import operator\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Task 1: Clustering Word2Vec embeddings\n",
    "\n",
    "---\n",
    "\n",
    "In this first task we will explore clustering on a larger scale. We will work with 6000 words. First you need to load the embeddings and the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings and mapping from words to matrix rows\n",
    "embeddings, mapping = plot_utils.get_embeddings()\n",
    "\n",
    "# Get the 6000 most frequent words and sort by row id\n",
    "words = [ w for w, i in sorted(mapping.items(), key=operator.itemgetter(1)) if i < 6000 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if you run the code cell below, you will see all the words plotted, color-coded by cluster. It is obviously impossible to put word labels on this many points without making the figure unreadable. The visualization might still cointain some interesting information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "n_clusters = 3\n",
    "\n",
    "# Initialize algorithm and perform k-means on embeddings\n",
    "model = KMeans(n_clusters)\n",
    "model = model.fit(embeddings)\n",
    "    \n",
    "# Plot clusters\n",
    "plot_utils.plot_kmeans(model, words, embeddings, mapping, plot_text=False, small_points=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**. `(0.5 points)` The example code above performs k-means with three clusters. Rerun the code cell above a few times to see how robust the three clusters are. **Do they change in different runs?** Note that the colors might change every time you run the cell; what we mean by changing here is that the points are clearly grouped differently. Answer the question as a comment in the dedicated code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer to 1.1 goes here\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show you how you can randomly sample words from each cluster. In the example code we sample 100 words (`show_n = 100`) from the clusters 0-2 (`show_clusters = [0, 1, 2]`). Notice that the clusters are given as integers in a list. Because of random sampling you will get different words for each cluster every time you run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n = 100\n",
    "show_clusters = [0, 1, 2]\n",
    "\n",
    "plot_utils.sample_clusters(model, words, show_n, show_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**. `(0.5 points)` Rerun the sampling in the code cell above multiple times to get a good picture of the words in each cluster. **Do the three clusters represent meaningful groups or is is hard to see any connections? How do you come to the conclusion?** Answer the question as a comment in the dedicated code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer to 1.2 goes here\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**. `(1 point)` In this third step you should explore different numbers of clusters. Go back up and change the line `n_clusters = 3`. Rerun the cell to get a new clustering. The number can be anything from a few clusters to tens or hundreds of them. Try at least **three** different numbers. **For each clustering, sample some of the clusters and see if you can find meaningful connections between the words. Answer as comments whether the clusters correspond to some sensible categories (these might be for example semantic or syntactic) and how you come to that conclusion.** \n",
    "\n",
    "*Note 1: You do not have to sample words for every cluster when you have tens or hundreds of them. Just try to get a good overview of the quality.*\n",
    "\n",
    "*Note 2: The more clusters you use, the slower it is to run the code. If it is unbearably slow, reduce the number of clusters. It works fine (takes a few minutes to run) with up to a few hundred clusters, at least.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 1.3 as comments here\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Task 2: Training and clustering embeddings\n",
    "\n",
    "---\n",
    "\n",
    "Run the code cell below to import some more necessary stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import two book texts from the Gutenberg corpus: Moby Dick and Sense and Sensibility\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "moby_dick = Text(gutenberg.words(\"melville-moby_dick.txt\"))\n",
    "sense_and_sensibility = Text(gutenberg.words(\"austen-sense.txt\"))\n",
    "\n",
    "import distribsem\n",
    "\n",
    "# Here we filter out some characters from the texts to make them less noisy\n",
    "moby_dick = distribsem.filter_text(moby_dick)\n",
    "sense_and_sensibility = distribsem.filter_text(sense_and_sensibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will train embeddings on our own text. As you might guess, you have two choices; you can use either *Moby Dick* or *Sense and Sensibility*. The first code cell below contains the code for training the embeddings, performing the clustering and plotting the clusters. The next code cell samples words from the clusters.\n",
    "\n",
    "In the home assignment of Lab 2, we examined the word contexts and tried to figure out how different window sizes and dimensionalities affect the resulting word embeddings. This time we will examine how the different parameters affect the clustering.\n",
    "\n",
    "The parameters we are interested in are `n_clusters` (as in Task 1 above) as well as `dimensionality` and `window_size`. The parameter `dimensionality` determines the size of the context vocabulary (and consequently the dimensionality of the embeddings). The value of `window_size` defines how many words we will take into account on each side of the target word. For the value of `text` you can choose either `moby_dick` or `sense_and_sensibility`. The embeddings are trained on the text you pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "n_clusters = 4                # Try different values here\n",
    "\n",
    "# Create embeddings\n",
    "embeddings, mapping = distribsem.create_vectors(\n",
    "    dimensionality = 1000,    # Try different values here\n",
    "    window_size = 4,          # Try different values here\n",
    "    text = moby_dick          # Choose either text for your study: moby_dick or sense_and_sensibility\n",
    ")\n",
    "\n",
    "# Perform clustering\n",
    "model = KMeans(n_clusters)\n",
    "model = model.fit(embeddings)\n",
    "    \n",
    "# Plot clusters\n",
    "vocabulary = [w for w, i in sorted(mapping.items(), key=operator.itemgetter(1))]\n",
    "plot_utils.plot_kmeans(model, vocabulary, embeddings, mapping, plot_text=False, small_points=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n = 50\n",
    "show_clusters = [0, 1, 2, 3]\n",
    "\n",
    "plot_utils.sample_clusters(model, vocabulary, show_n, show_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing the task try at least **two different dimensionality/window size-combinations**. For each combination, try out different numbers of clusters like in Task 1 above. Answer the following questions as comments in the dedicated code cell below: \n",
    "\n",
    "**2.1.** How do the dimensionalities/window sizes affect the resulting clustering?\n",
    "\n",
    "**2.2.** Are the differences easier to see with a small or a large number of clusters?\n",
    "\n",
    "**2.3.** Can you find an optimal number of clusters for some combination?\n",
    "\n",
    "**2.4.** How does the quality of the clustering compare to that of the W2V embeddings in Task 1?\n",
    "\n",
    "You can get a maximum of 2 points from this task. Each subtask is worth 0.5 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer the questions (2.1 – 2.4) as comments here\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download this page with your additions as a Notebook file (.ipynb) and return through Moodle.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
