{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 3, Part 3: Compositional distributional lexical semantics\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"red\">**This page contains interactive graphics. It only works properly if you change to the \"classic notebook\" user interface. Start by selecting *Launch Classic Notebook* from the *Help* menu.**</font>\n",
    "\n",
    "---\n",
    "\n",
    "The final topic of today's lab is compositionality of meaning elements within words.\n",
    "\n",
    "First, remember to import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append(\"../../../sem-prag-2025/src\")\n",
    "import plot_utils\n",
    "\n",
    "embeddings, mapping = plot_utils.get_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1: Vector arithmetic with Word2Vec\n",
    "\n",
    "In the slides we saw some interesting properties of W2V- and GloVe-embeddings under the heading *Compositional meaning in Word2Vec and GloVe*. The examples show how the relations between the word embeddings in vector space match our intuitions; for example, subtracting the embedding for *man* from *king* and adding *woman* yields *queen* (in the optimal case).\n",
    "\n",
    "    king - man + woman = queen\n",
    "\n",
    "In this section we will try to visualize this process.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 3.1.1** Run the code cell below. You will see a single vector for the word *king*. Now change the function argument `minus` to `\"man\"` instead of `None`. You should see a red vector starting from the end point of the `king` vector. The red vector corresponds to `man`, but it points in the opposite direction of what plain `man` would do. The red vector thus corresponds to a vector for `-man`. If you now follow the vectors for `king` and `-man`, you will end up at the point for `king - man`. A yellow vector going directly from the origin to the end point of `king - man` is shown as well.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 3.1.2** Now change the argument `plus` to `\"woman\"`. You should see a new blue vector, which is the vector for `woman`. The blue vector starts at the end point of `king - man`. Again, there is a yellow vector, which points at the final result:\n",
    "\n",
    "    king - man + woman\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 3.1.3** Change the argument `results` to `[ \"queen\", \"princess\", \"prince\", \"maid\" ]`. Now you will see some dots indicating where these four words are *actually* in this two-dimensional projection of the word2vec space. If the compositionality works as it should, the yellow vector should point towards the point labeled *queen*. Is that what happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_utils.plot_w2v_algebra(\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    base=\"king\",\n",
    "    minus=None,\n",
    "    plus=None,\n",
    "    results=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 3.1.4** In the code cell below, try out vector arithmetic on some words of your own choice. Try to figure out equations of the following form:\n",
    "\n",
    "    king - man + woman = queen\n",
    "    paris - france + germany = berlin\n",
    "    bigger - big + small = smaller\n",
    "    greece - warm + cold = ?\n",
    "\n",
    "Do the result make any sense? Evaluating these things is not that simple when all you have is a (two-dimensional) figure. You can decide the number of words that you put in the `results` field.\n",
    "\n",
    "Leave a nice result in the code cell and continue to the next exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_w2v_algebra(\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    base=\"???\",      # change this\n",
    "    minus=None,      # change this\n",
    "    plus=None,       # change this\n",
    "    results=[]       # change this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 3.1.5** In the next code cell, find some good example of *additive* compositionality, that is you do not have any \"minus\" word at all.\n",
    "\n",
    "    word1 + word2 = ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_w2v_algebra(\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    base=\"???\",      # change this\n",
    "    minus=None,      # don't change this line here\n",
    "    plus=None,       # change this\n",
    "    results=[]       # change this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 3.1.6** In the next code cell, find some good example of *subtractive* compositionality, that is you do not have any \"plus\" word at all. \n",
    "\n",
    "    word1 - word2 = ?\n",
    "\n",
    "(In this exercise, please do not just reverse some of the compositions you found above for additive compositionality.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_w2v_algebra(\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    base=\"???\",      # change this\n",
    "    minus=None,      # change this\n",
    "    plus=None,       # don't change this line here\n",
    "    results=[]       # change this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 3.1.7** Try to find some examples of *prejudice* or *bias* in the word2vec embeddings. The bias can be related, for instance, to gender, race or ethnicity, such as:\n",
    "\n",
    "    doctor - man + woman = nurse\n",
    "    \n",
    "Come up with your own example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_w2v_algebra(\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    base=\"doctor\",      # change this\n",
    "    minus=\"man\",        # change this\n",
    "    plus=\"woman\",       # change this\n",
    "    results=[ \"nurse\", \"professor\", \"gynecologist\",\n",
    "              \"midwife\", \"wife\", \"scientist\", \"nanny\" ] # change this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this you can continue with the home assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
