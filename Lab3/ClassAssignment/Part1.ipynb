{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 3, Part 1: Clustering\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"red\">**This page contains interactive graphics. It only works properly if you change to the \"classic notebook\" user interface. Start by selecting *Launch Classic Notebook* from the *Help* menu.**</font>\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, we will continue using word2vec (W2V) word embeddings.\n",
    "\n",
    "Start by running the code in the cell below. This code imports the necessary module for plotting vectors and for clustering. The code also intializes the word2vec word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append(\"../../../sem-prag-2025/src\")\n",
    "import plot_utils\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "\n",
    "embeddings, mapping = plot_utils.get_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start the lab session by taking a look at **clustering**. We will try out two different clustering methods that were introduced in the lecture. The first one is **k-means** and the second one is **hierarchical (agglomerative)** clustering.\n",
    "\n",
    "---\n",
    "\n",
    "### Section 1.1: Flat clustering \n",
    "\n",
    "---\n",
    "\n",
    "The first method we will look at is a \"flat\" clustering algorithm called k-means. Flat in this case means that the resulting clusters do not have any explicit structure. The optimal result is simply that words within a cluster are maximally similar to each other, while words in different clusters are maximally different from each other. We saw a [demo](http://shabal.in/visuals/kmeans/1.html) of how k-means clustering works; if you need a refresher you can check that out again.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 1.1.1** In the code cell below we show you how to cluster a set of words and plot the results. You only need to worry about what words to be clustered and how many clusters there should be (`words = ...`, `clusters = ...`). \n",
    "\n",
    "Try out different words and numbers of clusters and think about the following questions: \n",
    "\n",
    "- How well does the clustering work?\n",
    "- Which words seem to work best?\n",
    "- What kind of categories do you think the clusters represent?\n",
    "- Is there a number of clusters that gives sensible results most of the time, or one that doesn't work at all?\n",
    "- Do you see any problems with having to define the number of clusters yourself?\n",
    "- Do the clusters change when you run the algorithm several times?\n",
    "\n",
    "Note that here the *colors* indicate the clusters. The two-dimensional projection is not in itself a reliable view of which words are close to each other in this task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the words to be clustered and plotted\n",
    "words = \"run jump swim walk go take cry laugh speak talk hear\".split()\n",
    "clusters = 2\n",
    "\n",
    "# Represent the words in a suitable way for the clustering algorithm\n",
    "X = plot_utils.to_feature_matrix(words, embeddings, mapping)\n",
    "# Initialize clustering algorithm\n",
    "model = KMeans(n_clusters=clusters)\n",
    "# Train model\n",
    "model = model.fit(X)\n",
    "    \n",
    "# Plot results\n",
    "plot_utils.plot_kmeans(model, words, embeddings, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment on your observations (1.1.1) here:\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 1.2: Hierarchical clustering\n",
    "\n",
    "---\n",
    "\n",
    "In this section we will look at the second method: **hierarchical agglomerative clustering**. The method is hierarchical because it gives us a hierarchy of clusters instead of the flat, structureless clusters of k-means. We can investigate the hierarchy at different depths, resulting in different clusters depending on the level where we decide to group the words. Agglomerative means that the algorithm works in a bottom-up manner. Initially, each word is considered its own cluster. The clusters are then iteratively merged until we end up with one cluster. This results in the hierarchical structure.\n",
    "\n",
    "All of this is easier to see in a dendrogram. Run the code cell below to see the results.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 1.2.1** Again, try out different words. This time the number of clusters isn't the most important thing. As you might notice, the number you define doesn't change the structure of the resulting hierachy. What it changes is the depth where the algorithm groups the words into clusters. These clusters are shown after the word labels (`word/cluster_id`). Think about the following questions:\n",
    "\n",
    "- Do you see any potential benefits to using a hierarchical clustering instead of a flat one like k-means? Any problems? \n",
    "- Do the resulting clusters from this method match on to the clusters produced by k-means?\n",
    "- Do the clusters change when you run the algorithm multiple times?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the words to be clustered and plotted\n",
    "words = \"run jump swim walk go take cry laugh speak talk hear\".split()\n",
    "clusters = 4\n",
    "\n",
    "# Represent the words in a suitable way for the clustering algorithm\n",
    "X = plot_utils.to_feature_matrix(words, embeddings, mapping)\n",
    "# Initialize clustering algorithm\n",
    "model = AgglomerativeClustering(n_clusters=clusters)\n",
    "\n",
    "# Train model\n",
    "model = model.fit(X)\n",
    "    \n",
    "# Plot results\n",
    "plot_utils.plot_dendrogram(model, labels=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment on your observations (1.2.1) here:\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1.2.2** In this exercise you have a chance to make sure you understand how the final clusters can be determined from the dendrogram. \n",
    "\n",
    "In the code cell below you are given a function `get_clusters_at_cutoff`, that takes the clustering `model`, the source words and a cutoff value as arguments. It will print out the clusters that we would get if we cut off the hierarchy at the depth determined by the value *(Note: Depth in this case is calculated \"bottom-up\")*. \n",
    "\n",
    "In the example code we use a cutoff value 9. In that case all the words end up in a single cluster.\n",
    "\n",
    "Now make sure you understand how that is determined. Look at the dendrogram above, and try to predict what kind of clusters form at a certain depth. Then use the function below to check if you predicted correctly. \n",
    "\n",
    "Try this with a few different sets of words and cutoff values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.get_clusters_at_cutoff(model, words, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this you can continue to Part 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
