{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 3, Part 2: Compositional distributional semantics\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"red\">**This page contains interactive graphics. It only works properly if you change to the \"classic notebook\" user interface. Start by selecting *Launch Classic Notebook* from the *Help* menu.**</font>\n",
    "\n",
    "---\n",
    "\n",
    "### Section 2.1: Sentence embeddings\n",
    "\n",
    "In this part we will take a quick look at phrase/sentence embeddings. As is turns out, *averaging the embeddings of the words in a phrase* is one of the simplest ways of creating an embedding for the phrase as a whole. \n",
    "\n",
    "For example, let's imagine we have four words with the following embeddings:\n",
    "\n",
    "    the       = [1, 1, 2, 1]\n",
    "    cat       = [2, 0, 1, 3]\n",
    "    is        = [1, 0, 2, 0]\n",
    "    beautiful = [4, 1, 0, 0]\n",
    "    \n",
    "The embedding for the phrase *the cat is beautiful* could be calculated by averaging the four vectors:\n",
    "\n",
    "    ( the + cat + is + beautiful ) / 4\n",
    "    = ( [1, 1, 2, 1] + [2, 0, 1, 3] + [1, 0, 2, 0] + [4, 1, 0, 0] ) / 4\n",
    "    = [ (1 + 2 + 1 + 4), (1 + 0 + 0 + 1), (2 + 1 + 2 + 0), (1 + 3 + 0 + 0) ] / 4\n",
    "    = [ 8, 2, 5, 4 ] / 4\n",
    "    = [ 8/4, 2/4, 5/4, 4/4 ]\n",
    "    = [ 2, 0.5, 1.25, 1 ]\n",
    "    \n",
    "There are a multitude of ways to improve on this simple baseline, but given good word embeddings trained on a very large corpus, this method works surprisingly well on many different tasks. \n",
    "\n",
    "In the code cell below you can try out visualizing embeddings for different sentences. The intuition that guided our thinking with word vectors works here too: Similar sentences should have similar embeddings.\n",
    "\n",
    "Note that the visualizations are in two or three dimensions, whereas the original dimensionality of the vectors is 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append(\"../../../sem-prag-2025/src\")\n",
    "import plot_utils\n",
    "\n",
    "embeddings, mapping = plot_utils.get_embeddings()\n",
    "\n",
    "sents = [\"the man saw the queen\",\n",
    "         \"the king kissed the queen\",\n",
    "         \"the boat was really fast\",\n",
    "         \"the dog ran very fast\",\n",
    "         \"the princess was trying to hug the prince\",\n",
    "         \"the dog chased the cat\",\n",
    "         \"i like to watch cat videos\",\n",
    "         \"the royal family moved to a new palace\",\n",
    "         \"the animals were running around\",\n",
    "         ]\n",
    "\n",
    "plot_utils.plot_sentences_2d(sents, embeddings, mapping)\n",
    "plot_utils.plot_sentences_3d(sents, embeddings, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 2.1.1** The figures above are projections of high-dimensional vectors into two or three dimensions. We can also compare the similarities (or distances) between vectors directly in the original high-dimensional space by computing the angle between the vectors. These angles cannot be visualized as such, but the values can be used as measurements. The smaller the angle between two vectors, the more similar \"meanings\" the sentences are supposed to have.\n",
    "\n",
    "In the code cell below, you can enter a sentence and compare it to all the other sentences that were shown in the above plots. Try some different sentences and see how the distances change.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the target sentence to the above sentences\n",
    "target_sent = \"a queen saw the man\"\n",
    "\n",
    "plot_utils.rank_sentences_by_similarity(target_sent, sents, embeddings, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 2.1.2** In the cell below answer the following questions:\n",
    "\n",
    "1. Did the measured sentence similarities make sense?\n",
    "\n",
    "2. Can you think of an obvious drawback to this way of producing sentence embeddings by averaging word embeddings? Are there some types of sentences that obviously have different semantics, but for which the sentence embeddings do not reflect this difference properly?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer questions 1 and 2 of Exercise 2.1.2 here:\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can move on to Part 3 of the lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
